{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CS3793_5233_assignment3_toh271.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM8b9KVYsETT"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "Implement 2 different machine learning algorithms\n",
        "*   Stochastic Gradient Descent\n",
        "*   ID3 Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9apbZGptej6"
      },
      "source": [
        "# import all required libraries\n",
        "import pandas as pd\n",
        "import math\n",
        "from csv import reader\n",
        "import matplotlib.pyplot as plt\n",
        "import random as rand"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ4GvcruktJb",
        "outputId": "b54b8ea9-662f-4fa9-f36e-e10f2326f0c8"
      },
      "source": [
        "#mount Google Drive to Notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdqXyFZ95P0j"
      },
      "source": [
        "# Assume that the data files are in the following folder -- THIS WILL BE USED BY THE TA\n",
        "basePath = \"/content/drive/My Drive/Colab Notebooks/Artificial Intelligence/Data/\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeYRnesWqvLm"
      },
      "source": [
        "#Stochastic Gradient Descent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XoSqVJG5FkG"
      },
      "source": [
        "# Data file name variables\n",
        "train = basePath + \"gd-train.dat\"\n",
        "test = basePath + \"gd-test.dat\"\n",
        "\n",
        "# Read the training and testing data files\n",
        "training = []\n",
        "testing = []\n",
        "\n",
        "#open file in read\n",
        "with open(train, 'r') as training_file:\n",
        "        #read data in separated by tabs\n",
        "        data = reader(training_file, delimiter='\\t')\n",
        "        #for each row in the data\n",
        "        for row in data:\n",
        "          #if row is not true\n",
        "          if not row:\n",
        "            #continue \n",
        "            continue\n",
        "           #append the row to training list\n",
        "          training.append(row)\n",
        "\n",
        "#open file in read\n",
        "with open(test, 'r') as testing_file:\n",
        "        #read data in separated by tabs\n",
        "        data = reader(testing_file, delimiter='\\t')\n",
        "        #for each row in the data\n",
        "        for row in data:\n",
        "          #if row is not true\n",
        "          if not row:\n",
        "            continue\n",
        "          testing.append(row)\n",
        "\n",
        "training_data = training[1:]\n",
        "testing_data = testing[1:]\n",
        "\n",
        "# Activation Function - implement Sigmoid\n",
        "def activation_function(h):\n",
        "    # given 'h' compute and return 'z' based on the activation function implemented\n",
        "  return 1 / (1 + math.exp(-h))\n",
        "\n",
        "# Train the model using the given training dataset and the learning rate\n",
        "# return the \"weights\" learnt for the perceptron - include the weight assocaited with bias as the last entry\n",
        "def train(train_data, learning_rate=0.5):\n",
        "    # initialize weights to 0\n",
        "    weights = []\n",
        "    #append +1 0 to weight \n",
        "    for i in range(13+1):\n",
        "      weights.append(0) \n",
        "\n",
        "    # go through each training data instance\n",
        "    for data in range(len(train_data)):\n",
        "        # get 'x' as one multi-variate data instance and 'y' as the ground truth class label\n",
        "        train_data[data] = [int(train_data[data][i]) for i in range(len(train_data[data]))]\n",
        "        x = train_data[data]\n",
        "        #x.append(1) #append bias\n",
        "        #print(x)\n",
        "        y = train_data[data][-1]\n",
        "        #print(y)\n",
        "        # obtain h(x)\n",
        "        h = sum([a * b for a, b in zip(x, weights)])\n",
        "        # call the activation function with 'h' as parameter to obtain 'z'\n",
        "        z = activation_function(h)\n",
        "        # update all weights individually using learning_rate, (y-z), and the corresponding 'x'\n",
        "        error = y-z\n",
        "        weights[-1] = weights[-1] + learning_rate * error \n",
        "        for i in range(len(weights)-1):\n",
        "          weights[i] = (weights[i] + (learning_rate)* (error))\n",
        "        # return the final learnt weights\n",
        "    return weights\n",
        "    \n",
        "# Test the model (weights learnt) using the given test dataset\n",
        "# return the accuracy value\n",
        "def test(test_data, weights, threshold):\n",
        "    # go through each testing data instance\n",
        "    count = 0 \n",
        "    for data in range(len(test_data)):\n",
        "        # get 'x' as one multi-variate data instance and 'y' as the ground truth class label\n",
        "        test_data[data] = [int(test_data[data][i]) for i in range(len(test_data[data]))]\n",
        "        x = test_data[data]\n",
        "        #x.append(1) #aapend bias\n",
        "        #print(x)\n",
        "        y = test_data[data][-1]\n",
        "        #print(\"y\",y)\n",
        "        #print(\"row \",test_data[data])\n",
        "        # obtain h(x)\n",
        "        h = sum([a * b for a, b in zip(x, weights)])\n",
        "        # call the activation function with 'h' as parameter to obtain 'z'\n",
        "        z = activation_function(h)\n",
        "        # use 'threshold' to convert 'z' to either 0 or 1 so as to match to the ground truth binary labels\n",
        "        predicted = predict(threshold,z)\n",
        "        #print(\"predicted\", predicted, z)\n",
        "        # compare thresholded 'z' with 'y' to calculate the positive and negative instances for calculating accuracy\n",
        "        if y == predicted:\n",
        "            count+=1\n",
        "    # return the accuracy value for the given test dataset\n",
        "    return count/len(test_data)*100\n",
        "  \n",
        "# Gradient Descent function\n",
        "def gradient_descent(df_train, df_test, learning_rate=0.5, threshold=0.5):\n",
        "    # call the train function to train the model and obtain the weights\n",
        "    weights = train(df_train, learning_rate)\n",
        "    #print(weights)\n",
        "    # call the test function with the training dataset to obtain the training accuracy\n",
        "    train_accuracy = test(df_train, weights, threshold)\n",
        "    # call the test function with the testing dataset to obtain the testing accuracy\n",
        "    test_accuracy = test(df_test, weights, threshold)\n",
        "    # return (trainAccuracy, testAccuracy)\n",
        "    return (train_accuracy, test_accuracy)\n",
        "\n",
        "# Threshold of 0.5 will be used to classify the instance for the test. If the value is >= 0.5, classify as 1 or else 0.\n",
        "def predict(threshold, z):\n",
        "  if z > threshold:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdPwgSBOtb1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9beedda8-c0f6-43a3-ee23-75d87645338a"
      },
      "source": [
        "learning_rate = 0\n",
        "threshold = 0.5\n",
        "lr_train = []\n",
        "acc_train = []\n",
        "lr_test = []\n",
        "acc_test = []\n",
        "\n",
        "# Main algorithm loop\n",
        "while learning_rate <=1:\n",
        "    # Loop through all the different learning rates [0.05, 1]\n",
        "    learning_rate+=.05\n",
        "\n",
        "    # For each learning rate selected, call the gradient descent function to obtain the train and test accuracy values\n",
        "    training_accuracy, testing_accuracy = gradient_descent(training_data, testing_data, learning_rate, threshold)\n",
        "    lr_train.append(learning_rate)\n",
        "    acc_train.append(training_accuracy)\n",
        "    lr_test.append(learning_rate)\n",
        "    acc_test.append(testing_accuracy)\n",
        "\n",
        "    # Print both the accuracy values as \"Accuracy for LR of 0.1 on Training set = x %\" OR \"Accuracy for LR of 0.1 on Testing set = x %\"\n",
        "    print(\"Accuracy for Learning Rate, \", \"{:.2f}\".format(learning_rate), \"results in Training Data \", training_accuracy)\n",
        "    print(\"Accuracy for Learning Rate, \", \"{:.2f}\".format(learning_rate), \"results in Testing Data \", testing_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for Learning Rate,  0.05 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.05 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.10 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.10 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.15 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.15 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.20 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.20 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.25 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.25 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.30 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.30 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.35 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.35 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.40 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.40 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.45 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.45 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.50 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.50 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.55 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.55 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.60 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.60 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.65 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.65 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.70 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.70 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.75 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.75 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.80 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.80 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.85 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.85 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.90 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.90 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  0.95 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  0.95 results in Testing Data  72.25\n",
            "Accuracy for Learning Rate,  1.00 results in Training Data  68.0\n",
            "Accuracy for Learning Rate,  1.00 results in Testing Data  72.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onnqJYTEq0l3"
      },
      "source": [
        "#ID3 Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alwR8nppBlqs"
      },
      "source": [
        "Sources used to create ID3 <br>\n",
        "ID3 from Scratch Towards Data Science : https://towardsdatascience.com/id3-decision-tree-classifier-from-scratch-in-python-b38ef145fd90\n",
        "\n",
        "Tutorials Decision Tree : https://github.com/random-forests/tutorials/blob/master/decision_tree.py\n",
        "\n",
        "Machine Learning with Python: Decision Trees in Python: https://www.python-course.eu/Decision_Trees.php\n",
        "\n",
        "\n",
        "ID3_From_Scratch : https://github.com/bergr7/ID3_From_Scratch/blob/main/ID3.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y0gmjfmu5iF"
      },
      "source": [
        "def InfoGain(data):\n",
        "  total_entropy = entropy(data)\n",
        "  conditional_entropy = 0\n",
        "  #n = total number of rows\n",
        "  n = len(data)\n",
        "\n",
        "  #for v in values_in_given_feature:\n",
        "  for v in range(len(data[0])-1):\n",
        "    for i in range(len(data)-1):\n",
        "      #list_of_values.append(data[i][v])\n",
        "      #valuesUnique = np.unique(list_of_values)        \n",
        "      #m = number of rows belonging to v\n",
        "      m = len(data)\n",
        "      #sub_data for v\n",
        "      subdata = data[i][v]\n",
        "      entropy(sub_data)\n",
        "      conditional_entropy += m(entropy/n)\n",
        "\n",
        "  return total_entropy - conditional_entropy\n",
        "\n",
        "\n",
        "#Records a 'column number' and a'column value' \n",
        "#match function  is used to comparethe feature value in an example to the feature value stored in the question.\n",
        "class Question:\n",
        "    def __init__(self, column, value):\n",
        "        self.column = column\n",
        "        self.value = value\n",
        "    def match(self, example):\n",
        "        val = example[self.column]\n",
        "        if is_numeric(val):\n",
        "            return val >= self.value\n",
        "        else:\n",
        "            return val == self.value\n",
        "    def __repr__(self):\n",
        "        condition = \"=\"\n",
        "        if is_numeric(self.value):\n",
        "            condition = \">=\"\n",
        "        return \"%s %s %s:\" % (\n",
        "            header[self.column], condition, str(self.value))\n",
        "        \n",
        "        \n",
        "# Keeps track number of timesit appears in the rows from the training data that reach this leaf in dict form.        \n",
        "class Leaf:\n",
        "    def __init__(self, rows, question):\n",
        "        self.predictions = class_counts(rows)\n",
        "        self.question = question\n",
        "\n",
        "# keeps reference to the question, and to the two child nodes.\n",
        "class Decision_Node:\n",
        "    def __init__(self,\n",
        "                 question,\n",
        "                 true_branch,\n",
        "                 false_branch):\n",
        "        self.question = question\n",
        "        self.true_branch = true_branch\n",
        "        self.false_branch = false_branch\n",
        "\n",
        "def entropy(data):\n",
        "  #n = total number of rows in data\n",
        "  n = len(data)\n",
        "  entropy_val = 0\n",
        "  count_0 = 0\n",
        "  count_1 = 0\n",
        "  for i in range(len(data)):\n",
        "    if data[i][-1] == 0:\n",
        "      count_0 += 1\n",
        "    else:\n",
        "      count_i += 1\n",
        "  #for c in classes(yes/no):\n",
        "  for c in range(len(data)):\n",
        "    if data[i][-1] == 0:\n",
        "      #m = number of rows belonging to class c\n",
        "      m = count_0\n",
        "      p = m/n\n",
        "      entropy_val += 0 if m==0 else(-p*np.log2(p))\n",
        "    else:\n",
        "      #m = number of rows belonging to class c\n",
        "      m = count_1\n",
        "      p = m/n\n",
        "      entropy_val += 0 if m==0 else(-p*np.log2(p))\n",
        "  return entropy_val\n",
        "\n",
        "  #is node/leaf a number or not\n",
        "def is_numeric(value):\n",
        "  return isinstance(value, int) or isinstance(value, float)\n",
        "\n",
        "#counts the number of each type of example in a dataset\n",
        "def class_counts(rows):\n",
        "  #Value output will be in some form like so {'0': 288, '1': 108}\n",
        "  counts = {} \n",
        "  for row in rows:\n",
        "    # in our dataset format, the label is always the last column\n",
        "    label = row[-1]\n",
        "    #if label is not in our dict of counts\n",
        "    if label not in counts:\n",
        "        #set to 0 to reset value \n",
        "        counts[label] = 0\n",
        "    #add 1 to label\n",
        "    counts[label] += 1\n",
        "  #return counts\n",
        "  return counts\n",
        "\n",
        "#aid in entropy\n",
        "def score(rows):\n",
        "  #return counts of data \n",
        "  counts = class_counts(rows)\n",
        "  entropy = 1\n",
        "  #for each label in counts\n",
        "  for lbl in counts:\n",
        "    #set the probability from value / # of rows in data\n",
        "      prob_of_lbl = counts[lbl] / float(len(rows))\n",
        "      #subtract from entropy\n",
        "      entropy -= prob_of_lbl**2\n",
        "  #return the entropy\n",
        "  return entropy\n",
        "\n",
        "#info_gain measure the uncertainty of the starting node, minus the weighted impurity of two child nodes\n",
        "def info_gain(left, right, current_uncertainty):\n",
        "  #get probability \n",
        "  p = float(len(left)) / (len(left) + len(right))\n",
        "  #return the info\n",
        "  return current_uncertainty - p * score(left) - (1 - p) * score(right)\n",
        "\n",
        "#partition the data by question\n",
        "def partition(rows, question):\n",
        "  true_rows, false_rows = [], []\n",
        "  #For each row in the dataset\n",
        "  for row in rows:\n",
        "    #check if it matches the question\n",
        "    if question.match(row):\n",
        "      #add it to 'true rows\n",
        "      true_rows.append(row)\n",
        "    #otherwise\n",
        "    else:\n",
        "      #add it to 'false rows\n",
        "      false_rows.append(row)\n",
        "  #return the true and false rows\n",
        "  return true_rows, false_rows\n",
        "\n",
        "#partitioning the data on each of the unique attribute, calculate the information gain, and return the question that produces the highest gain.\n",
        "def find_best_split(rows):\n",
        "  #inition way to keep track of best info gain\n",
        "  best_gain = 0 \n",
        "  best_question = None  \n",
        "  current_uncertainty = score(rows)\n",
        "  #get number of columns\n",
        "  n_features = len(rows[0]) - 1\n",
        "  #in each of the feature/attributes  \n",
        "  for col in range(n_features): \n",
        "    #get the unique values in the column\n",
        "    values = set([row[col] for row in rows])\n",
        "    #for each value in the values\n",
        "    for val in values:  \n",
        "      #get the question\n",
        "      question = Question(col, val)\n",
        "      #partition - split the data\n",
        "      true_rows, false_rows = partition(rows, question)\n",
        "      #skip the split if you cannot split\n",
        "      if len(true_rows) == 0 or len(false_rows) == 0:\n",
        "        continue\n",
        "      #get the info gain from the split\n",
        "      gain = info_gain(true_rows, false_rows, current_uncertainty)\n",
        "      #if the current gain is bigger or equal to the best gain we have\n",
        "      if gain >= best_gain:\n",
        "        #set the new best gain and best question to be our current\n",
        "        best_gain, best_question = gain, question\n",
        "  #return the best gain and question\n",
        "  return best_gain, best_question\n",
        "\n",
        "#build our tree\n",
        "def build_tree(rows):\n",
        "  #partitioning the data on each of the unique attribute, calculate the information gain, and return the question that produces the highest gain.\n",
        "  # Find the best attribute to split by calculating the maximum information gain from the attributes remaining by calculating the entropy\n",
        "  gain, question = find_best_split(rows)\n",
        "  # If you reach a leaf node in the decision tree and have no examples left or the examples are equally split among multiple classes\n",
        "  if gain == 0:\n",
        "      return Leaf(rows, question)\n",
        "  # If you reached a leaf node but still have examples that belong to different classes (there are no remaining attributes to be split) If we reach here, we have found a useful feature / value to partition on\n",
        "  true_rows, false_rows = partition(rows, question)\n",
        "  #Recursively build the true branch.\n",
        "  true_branch = build_tree(true_rows)\n",
        "  # Recursively build the false branch.\n",
        "  false_branch = build_tree(false_rows)\n",
        "  # Return a Question node which is the tree with the best feature / value as well as the branches \n",
        "  return Decision_Node(question, true_branch, false_branch)\n",
        "\n",
        "#print our tree\n",
        "def print_tree(node, spacing=\"\"):\n",
        "  # Base case: we've reached a leaf\n",
        "  if isinstance(node, Leaf):\n",
        "      print (spacing + \"attr\", print_leaf(node.predictions), end = ' ')\n",
        "      return\n",
        "  # Print the question at this node\n",
        "  print (spacing + str(node.question), end = ' ')\n",
        "  print (' ')\n",
        "  # Call this function recursively on the true branch\n",
        "  print_tree(node.true_branch, spacing + \" \")\n",
        "  print (' ')\n",
        "  # Call this function recursively on the false branch\n",
        "  print_tree(node.false_branch, spacing + \" \")\n",
        "\n",
        "\n",
        "def classify(row, node):\n",
        "  # Base case: we've reached a leaf\n",
        "  if isinstance(node, Leaf):\n",
        "      return node.predictions\n",
        "  #if the question matches then\n",
        "  if node.question.match(row):\n",
        "    #recuresively call the classify following true branch\n",
        "    return classify(row, node.true_branch)\n",
        "  else:\n",
        "    #recursively call classify following false branch\n",
        "    return classify(row, node.false_branch)\n",
        "\n",
        "def print_leaf(counts):\n",
        "  total = sum(counts.values()) * 1.0\n",
        "  probability = {}\n",
        "  for label in counts.keys():\n",
        "      #probability[label] = str(int(counts[label] / total * 100)) + \"%\"\n",
        "      probability[label] = int(counts[label])\n",
        "  return probability\n",
        "\n",
        "#print our accurancy\n",
        "def print_accuracy(rows, tree):\n",
        "  count = 0\n",
        "  length = len(rows)\n",
        "  #for row in data \n",
        "  for row in rows:\n",
        "    #get the results\n",
        "    result = classify(row, tree)\n",
        "    #set left value to be 0\n",
        "    left = result.get('0')\n",
        "    #if left is None\n",
        "    if left is None:\n",
        "      #est to 0\n",
        "      left=0\n",
        "    #get right value to be 1\n",
        "    right = result.get('1')\n",
        "    #if right is None\n",
        "    if right is None:\n",
        "      #set to 0\n",
        "      right=0\n",
        "    # if left is bigger than right\n",
        "    if left>right:\n",
        "      #set results to be 0\n",
        "      result=0\n",
        "    #otherwise right is bigger than left\n",
        "    else:\n",
        "      #set restuls to be 1\n",
        "      result=1\n",
        "    #if result is not equal to the class predictor\n",
        "    if result != int(row[-1]):\n",
        "      #add 1 to our count\n",
        "      count=count+1\n",
        "  #return accuracy \n",
        "  return ((length-count)/length * 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSUNcaVFvMKd",
        "outputId": "af4e5402-96a9-421b-9c5e-1eaabc74c480"
      },
      "source": [
        "# Data file name variables\n",
        "train = basePath + 'id3-train.dat'\n",
        "test = basePath + 'id3-test.dat'\n",
        "\n",
        "\n",
        "\n",
        "# Following is the base code structure. Feel free to change the code structure as you see fit, maybe even create more functions.\n",
        "# Data file name variables\n",
        "train = basePath + \"id3-train.dat\"\n",
        "test = basePath + \"id3-test.dat\"\n",
        "\n",
        "#test with weather data\n",
        "\n",
        "class Node(object):\n",
        "  def __init__(self, parent=None):\n",
        "    self.parent = parent\n",
        "    self.children = []\n",
        "    self.splitAttribute = None\n",
        "    self.splitAttributeValue = None\n",
        "    self.count = None\n",
        "    self.cls = None\n",
        "    \n",
        "# Read the first line in the training data file, to get the number of attributes\n",
        "# Read the training and testing data files\n",
        "training = []\n",
        "testing = []\n",
        "\n",
        "#open file in read\n",
        "with open(train, 'r') as training_file:\n",
        "        #read data in separated by tabs\n",
        "        data = reader(training_file, delimiter='\\t')\n",
        "        #for each row in the data\n",
        "        for row in data:\n",
        "          #if row is not true\n",
        "          if not row:\n",
        "            #continue \n",
        "            continue\n",
        "           #append the row to training list\n",
        "          training.append(row)\n",
        "\n",
        "#open file in read\n",
        "with open(test, 'r') as testing_file:\n",
        "        #read data in separated by tabs\n",
        "        data = reader(testing_file, delimiter='\\t')\n",
        "        #for each row in the data\n",
        "        for row in data:\n",
        "          #if row is not true\n",
        "          if not row:\n",
        "            continue\n",
        "          testing.append(row)\n",
        "header = training[0]\n",
        "training_data = training[1:]\n",
        "testing_data = testing[1:]\n",
        "\n",
        "# Read all the training instances and the ground truth class labels.\n",
        "# Create the decision tree by implementing the ID3 algorithm. Pseudocode provided above.\n",
        "# Print the tree in the example format mentioned.\n",
        "tree = build_tree(training_data)\n",
        "print_tree(tree)\n",
        "\n",
        "#build test tree\n",
        "testing_tree = build_tree(testing_data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attr5 = 0:  \n",
            " attr6 = 0:  \n",
            "  attr2 = 0:  \n",
            "   attr1 = 1:  \n",
            "    attr4 = 0:  \n",
            "     attr3 = 0:  \n",
            "      attr {'1': 1, '0': 14}  \n",
            "      attr {'0': 10}  \n",
            "     attr {'0': 30}  \n",
            "    attr4 = 1:  \n",
            "     attr3 = 1:  \n",
            "      attr {'0': 13, '1': 1}  \n",
            "      attr {'0': 12, '1': 1}  \n",
            "     attr3 = 0:  \n",
            "      attr {'0': 8, '1': 1}  \n",
            "      attr {'1': 3, '0': 9}  \n",
            "   attr4 = 0:  \n",
            "    attr3 = 0:  \n",
            "     attr {'0': 25}  \n",
            "     attr1 = 0:  \n",
            "      attr {'0': 11}  \n",
            "      attr {'0': 16, '1': 2}  \n",
            "    attr1 = 1:  \n",
            "     attr3 = 0:  \n",
            "      attr {'1': 4, '0': 8}  \n",
            "      attr {'0': 13, '1': 4}  \n",
            "     attr3 = 0:  \n",
            "      attr {'0': 9, '1': 1}  \n",
            "      attr {'0': 15, '1': 3}  \n",
            "  attr4 = 1:  \n",
            "   attr2 = 0:  \n",
            "    attr1 = 0:  \n",
            "     attr3 = 1:  \n",
            "      attr {'0': 11, '1': 1}  \n",
            "      attr {'0': 15, '1': 3}  \n",
            "     attr3 = 0:  \n",
            "      attr {'0': 10, '1': 3}  \n",
            "      attr {'1': 2, '0': 8}  \n",
            "    attr1 = 1:  \n",
            "     attr3 = 0:  \n",
            "      attr {'0': 8, '1': 1}  \n",
            "      attr {'0': 4, '1': 4}  \n",
            "     attr3 = 0:  \n",
            "      attr {'1': 9, '0': 2}  \n",
            "      attr {'1': 9}  \n",
            "   attr2 = 0:  \n",
            "    attr3 = 0:  \n",
            "     attr1 = 1:  \n",
            "      attr {'0': 11, '1': 2}  \n",
            "      attr {'0': 10, '1': 1}  \n",
            "     attr {'1': 4, '0': 14}  \n",
            "    attr1 = 0:  \n",
            "     attr3 = 1:  \n",
            "      attr {'0': 10, '1': 1}  \n",
            "      attr {'0': 8, '1': 2}  \n",
            "     attr3 = 1:  \n",
            "      attr {'0': 15, '1': 1}  \n",
            "      attr {'0': 13, '1': 1}  \n",
            " attr6 = 0:  \n",
            "  attr3 = 1:  \n",
            "   attr2 = 0:  \n",
            "    attr4 = 0:  \n",
            "     attr1 = 1:  \n",
            "      attr {'0': 2, '1': 12}  \n",
            "      attr {'1': 12, '0': 1}  \n",
            "     attr {'1': 28}  \n",
            "    attr1 = 0:  \n",
            "     attr4 = 0:  \n",
            "      attr {'1': 13, '0': 1}  \n",
            "      attr {'1': 8, '0': 1}  \n",
            "     attr4 = 0:  \n",
            "      attr {'0': 1, '1': 7}  \n",
            "      attr {'0': 13, '1': 1}  \n",
            "   attr4 = 0:  \n",
            "    attr1 = 0:  \n",
            "     attr2 = 0:  \n",
            "      attr {'0': 12, '1': 2}  \n",
            "      attr {'0': 11}  \n",
            "     attr2 = 0:  \n",
            "      attr {'0': 6, '1': 2}  \n",
            "      attr {'1': 3, '0': 11}  \n",
            "    attr1 = 0:  \n",
            "     attr2 = 0:  \n",
            "      attr {'1': 14, '0': 2}  \n",
            "      attr {'0': 3, '1': 17}  \n",
            "     attr2 = 0:  \n",
            "      attr {'0': 10, '1': 1}  \n",
            "      attr {'0': 8, '1': 1}  \n",
            "  attr4 = 1:  \n",
            "   attr1 = 1:  \n",
            "    attr2 = 0:  \n",
            "     attr3 = 1:  \n",
            "      attr {'1': 4, '0': 9}  \n",
            "      attr {'0': 9, '1': 1}  \n",
            "     attr3 = 0:  \n",
            "      attr {'0': 15, '1': 1}  \n",
            "      attr {'0': 13, '1': 1}  \n",
            "    attr2 = 0:  \n",
            "     attr3 = 0:  \n",
            "      attr {'0': 11}  \n",
            "      attr {'1': 13}  \n",
            "     attr3 = 0:  \n",
            "      attr {'1': 10, '0': 2}  \n",
            "      attr {'0': 14}  \n",
            "   attr2 = 0:  \n",
            "    attr3 = 0:  \n",
            "     attr1 = 0:  \n",
            "      attr {'0': 17, '1': 1}  \n",
            "      attr {'0': 10, '1': 1}  \n",
            "     attr {'0': 26}  \n",
            "    attr1 = 0:  \n",
            "     attr {'0': 24, '1': 4}  \n",
            "     attr {'0': 14} "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuB1zo_KJXU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46302922-0c06-4e81-cc14-e18cf5160bb0"
      },
      "source": [
        "print(\"Accuracy on the Training data = %f\" % (print_accuracy(training_data, tree)))\n",
        "print(\"Accuracy on the Testing data = %f\" % (print_accuracy(testing_data, testing_tree)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on the Training data = 89.375000\n",
            "Accuracy on the Testing data = 90.640394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYSK99zp5a7H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "7e8d0248-ea60-499c-ee7c-d33a98d58997"
      },
      "source": [
        "accuracy = []\n",
        "sample = []\n",
        "tree_list = []\n",
        "\n",
        "length = len(training_data)\n",
        "x = 40\n",
        "\n",
        "# For each 'x',\n",
        "while x < length:\n",
        "  #get list of random numbers from testing data - 40 \n",
        "  # Randomly select 'x' instances\n",
        "  tree_list = rand.sample(training_data,x)\n",
        "  #build tree\n",
        "  # Create the ID3 decision tree using those instances\n",
        "  my_tree = build_tree(tree_list)\n",
        "  #append accuracy to list\n",
        "  # Calculate the accuracy of the ID3 tree created on the Test data\n",
        "  accuracy.append(print_accuracy(testing_data, my_tree))\n",
        "  #append x to sample\n",
        "  sample.append(x)\n",
        "  #add 40 to x\n",
        "  # Loop through to select the number of instances 'x' in increments of 40\n",
        "  x+=40\n",
        "\n",
        "# Plot the learning curve using the accuracy values\n",
        "plt.plot(sample,accuracy)\n",
        "# Y-axis will be the accuracy in % on the Test data\n",
        "plt.ylabel('Accuracy')\n",
        "# X-axis will be the number of training instances used for creating the tree\n",
        "\n",
        "plt.xlabel('Sample Size')\n",
        "plt.suptitle(\"Accuracy vs. Sample Size\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}